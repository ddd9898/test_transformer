{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train setting\n",
    "BATCH_SIZE = 8 #2560\n",
    "LR = 0.0001 # 0.0001\n",
    "NUM_EPOCHS = 1000 #500\n",
    "MAX_LEN = 3\n",
    "DIM_EMBEDDING = 128\n",
    "MASK_IDX = 1 #0:5\n",
    "OUTPUT_IDX = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class autoencoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                dropout=0.2,\n",
    "                num_heads=8,\n",
    "                vocab_size = MAX_LEN,\n",
    "                d_embedding=32,\n",
    "                num_encoder_layers=1,\n",
    "                output_idx = 2\n",
    "                ):\n",
    "        super(autoencoder, self).__init__()\n",
    "        \n",
    "        self.output_idx = output_idx\n",
    "        \n",
    "        self.embeddingLayer_encoder = nn.Embedding(vocab_size, d_embedding)\n",
    "\n",
    "        ##Encoder \n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=d_embedding, nhead=num_heads,dim_feedforward=1024,dropout=dropout)\n",
    "        encoder_norm = nn.LayerNorm(d_embedding)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers,num_encoder_layers,encoder_norm)\n",
    "        \n",
    "        ##output\n",
    "        self.fc_out = nn.Linear(d_embedding, MAX_LEN)\n",
    "\n",
    "    def forward(self,src,src_mask):\n",
    "        \n",
    "        src = self.embeddingLayer_encoder(src)\n",
    "\n",
    "        #Encoder\n",
    "        src = src.permute(1,0,2) #seq * batch * feature\n",
    "        output = self.transformer_encoder(src=src,mask=src_mask)\n",
    "        output = output.permute(1,0,2) #batch * seq * feature\n",
    "        \n",
    "        output = self.fc_out(output)\n",
    "        \n",
    "        ##Attention\n",
    "        attn_output_weights = self.transformer_encoder.layers[0].self_attn(src, src, src,attn_mask=src_mask)[1]\n",
    "        # attn_output_weights = torch.sum(attn_output_weights,dim=1)\n",
    "\n",
    "         \n",
    "        return output[:,self.output_idx,:],attn_output_weights[:,self.output_idx,:]\n",
    "\n",
    "##创建模型，只输出transformer_encoder某个位置的输出结果和注意力权重\n",
    "device = \"cpu\"\n",
    "model = autoencoder(num_encoder_layers = 1,d_embedding=DIM_EMBEDDING,output_idx=OUTPUT_IDX).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_batch_data(batch_size = 0):\n",
    "    '''\n",
    "    Batch * len\n",
    "    '''\n",
    "    x = list()\n",
    "    y = list()\n",
    "    tokenizer = {\n",
    "                'A':0,\n",
    "                'B':1,\n",
    "                'C':2}\n",
    "    label_embedding = {'A':[1,0,0],\n",
    "                      'B': [0,1,0],\n",
    "                      'C': [0,0,1]}\n",
    "    for n in range(batch_size):\n",
    "        sample = list()\n",
    "        label = list()\n",
    "        for m in range(MAX_LEN):\n",
    "            char = random.choice(['A','B','C']) \n",
    "            sample.append(tokenizer[char])\n",
    "            label.append(label_embedding[char])\n",
    "        x.append(sample)\n",
    "        y.append(label)\n",
    "        \n",
    "    x = torch.tensor(x).type(torch.int64)\n",
    "    y = torch.tensor(y).type(torch.float32)\n",
    "    return x,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., -inf, 0.],\n",
      "        [0., -inf, 0.],\n",
      "        [0., -inf, 0.]])\n"
     ]
    }
   ],
   "source": [
    "##创建src_mask，遮挡输入序列中第MASK_IDX个位置的信息\n",
    "src_mask =  torch.zeros(MAX_LEN,MAX_LEN)\n",
    "src_mask[:,MASK_IDX] = 1\n",
    "src_mask = src_mask.masked_fill(src_mask == 1, float('-inf'))\n",
    "print(src_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.8%的样本被预测正确\n",
      "31.0%的样本被预测正确\n",
      "34.5%的样本被预测正确\n",
      "34.0%的样本被预测正确\n",
      "33.5%的样本被预测正确\n",
      "32.9%的样本被预测正确\n",
      "29.7%的样本被预测正确\n",
      "35.1%的样本被预测正确\n",
      "35.2%的样本被预测正确\n",
      "34.1%的样本被预测正确\n"
     ]
    }
   ],
   "source": [
    "###开始验证transformer_encoder的第OUTPUT_IDX能否看见第MASK_IDX个输入\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01) #0.01\n",
    "criterion = torch.nn.MSELoss()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    ##Train\n",
    "    model.train()\n",
    "    \n",
    "    #Get data\n",
    "    x,y = get_batch_data(BATCH_SIZE)\n",
    "    \n",
    "    #Calculate output\n",
    "    optimizer.zero_grad()\n",
    "    pre,attn_output_weights = model(x,src_mask)\n",
    "    \n",
    "    #Calculate loss\n",
    "    loss = criterion(pre,y[:,1,:])\n",
    "    \n",
    "    #Optimize the model\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    ##Test\n",
    "    if (epoch % 100) == 0:\n",
    "        model.eval()\n",
    "        #Get data\n",
    "        x,y = get_batch_data(1000)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pre,attn_output_weights = model(x,src_mask)\n",
    "            pre_idx = torch.argmax(pre,1)\n",
    "            gt = torch.argmax(y[:,1,:],1)\n",
    "            print('{:.1f}%的样本被预测正确'.format(torch.sum(pre_idx==gt)/10))\n",
    "            # print(torch.sum(attn_output_weights,dim=0))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "##修改src_mask，不再遮挡输入序列中第MASK_IDX个位置的信息\n",
    "src_mask =  torch.zeros(MAX_LEN,MAX_LEN)\n",
    "# src_mask[:,MASK_IDX] = 1\n",
    "src_mask = src_mask.masked_fill(src_mask == 1, float('-inf'))\n",
    "print(src_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.7%的样本被预测正确\n",
      "65.3%的样本被预测正确\n",
      "66.7%的样本被预测正确\n",
      "69.4%的样本被预测正确\n",
      "65.6%的样本被预测正确\n",
      "66.3%的样本被预测正确\n",
      "65.6%的样本被预测正确\n",
      "67.4%的样本被预测正确\n",
      "66.4%的样本被预测正确\n",
      "64.9%的样本被预测正确\n"
     ]
    }
   ],
   "source": [
    "###重新测试transformer_encoder的第OUTPUT_IDX能否看见第MASK_IDX个输入\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01) #0.01\n",
    "criterion = torch.nn.MSELoss()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    ##Train\n",
    "    model.train()\n",
    "    \n",
    "    #Get data\n",
    "    x,y = get_batch_data(BATCH_SIZE)\n",
    "    \n",
    "    #Calculate output\n",
    "    optimizer.zero_grad()\n",
    "    pre,attn_output_weights = model(x,src_mask)\n",
    "    \n",
    "    #Calculate loss\n",
    "    loss = criterion(pre,y[:,1,:])\n",
    "    \n",
    "    #Optimize the model\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    ##Test\n",
    "    if (epoch % 100) == 0:\n",
    "        model.eval()\n",
    "        #Get data\n",
    "        x,y = get_batch_data(1000)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pre,attn_output_weights = model(x,src_mask)\n",
    "            pre_idx = torch.argmax(pre,1)\n",
    "            gt = torch.argmax(y[:,1,:],1)\n",
    "            print('{:.1f}%的样本被预测正确'.format(torch.sum(pre_idx==gt)/10))\n",
    "            # print(torch.sum(attn_output_weights,dim=0))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., -inf, 0.],\n",
      "        [0., -inf, 0.],\n",
      "        [0., -inf, 0.]])\n",
      "45.5%的样本被预测正确\n",
      "100.0%的样本被预测正确\n",
      "100.0%的样本被预测正确\n",
      "100.0%的样本被预测正确\n",
      "100.0%的样本被预测正确\n",
      "100.0%的样本被预测正确\n",
      "100.0%的样本被预测正确\n",
      "100.0%的样本被预测正确\n",
      "100.0%的样本被预测正确\n",
      "100.0%的样本被预测正确\n",
      "随便输出一个样本的注意力权重看看被遮挡位置的注意力权重是否为0\n",
      "tensor([0.2055, 0.0000, 0.6865], grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "###测试src_mask对角线遮挡是否有用\n",
    "src_mask =  torch.zeros(MAX_LEN,MAX_LEN) \n",
    "src_mask[:,MASK_IDX] = 1 ##创建src_mask，遮挡输入序列中第MASK_IDX个位置的信息\n",
    "src_mask = src_mask.masked_fill(src_mask == 1, float('-inf'))\n",
    "print(src_mask)\n",
    "\n",
    "#重新创建模型\n",
    "OUTPUT_IDX = 1\n",
    "model = autoencoder(num_encoder_layers = 1,d_embedding=DIM_EMBEDDING,output_idx=OUTPUT_IDX).to(device)\n",
    "\n",
    "###重新测试transformer_encoder的第OUTPUT_IDX能否看见第MASK_IDX个输入\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01) #0.01\n",
    "criterion = torch.nn.MSELoss()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    ##Train\n",
    "    model.train()\n",
    "    \n",
    "    #Get data\n",
    "    x,y = get_batch_data(BATCH_SIZE)\n",
    "    \n",
    "    #Calculate output\n",
    "    optimizer.zero_grad()\n",
    "    pre,attn_output_weights = model(x,src_mask)\n",
    "    \n",
    "    #Calculate loss\n",
    "    loss = criterion(pre,y[:,1,:])\n",
    "    \n",
    "    #Optimize the model\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    ##Test\n",
    "    if (epoch % 100) == 0:\n",
    "        model.eval()\n",
    "        #Get data\n",
    "        x,y = get_batch_data(1000)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pre,attn_output_weights = model(x,src_mask)\n",
    "            pre_idx = torch.argmax(pre,1)\n",
    "            gt = torch.argmax(y[:,1,:],1)\n",
    "            print('{:.1f}%的样本被预测正确'.format(torch.sum(pre_idx==gt)/10))\n",
    "            # print(torch.sum(attn_output_weights,dim=0))\n",
    "print('随便输出一个样本的注意力权重看看被遮挡位置的注意力权重是否为0')\n",
    "print(attn_output_weights[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([0.3422, 0.3369, 0.3209])\n",
      "tensor([0.3440, 0.3425, 0.3136])\n",
      "tensor([0.3456, 0.3459, 0.3085])\n",
      "tensor([0.3493, 0.3474, 0.3033])\n",
      "tensor([0.3502, 0.3465, 0.3033])\n"
     ]
    }
   ],
   "source": [
    "#测试对TransformerEncoderLayer输出序列的某个位置而言，所有没有被遮挡掉的输入序列位置的注意力权重都非常相近\n",
    "##修改src_mask，不再遮挡输入序列中第MASK_IDX个位置的信息\n",
    "src_mask =  torch.zeros(MAX_LEN,MAX_LEN)\n",
    "src_mask = src_mask.masked_fill(src_mask == 1, float('-inf'))\n",
    "print(src_mask)\n",
    "\n",
    "#重新创建模型\n",
    "OUTPUT_IDX = 2\n",
    "model = autoencoder(num_encoder_layers = 1,d_embedding=DIM_EMBEDDING,output_idx=OUTPUT_IDX).to(device)\n",
    "\n",
    "\n",
    "###重新测试transformer_encoder的第OUTPUT_IDX能否看见第MASK_IDX个输入\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01) #0.01\n",
    "criterion = torch.nn.MSELoss()\n",
    "for epoch in range(500):\n",
    "    ##Train\n",
    "    model.train()\n",
    "    \n",
    "    #Get data\n",
    "    x,y = get_batch_data(BATCH_SIZE)\n",
    "    \n",
    "    #Calculate output\n",
    "    optimizer.zero_grad()\n",
    "    pre,attn_output_weights = model(x,src_mask)\n",
    "    \n",
    "    #Calculate loss\n",
    "    loss = criterion(pre,y[:,1,:])\n",
    "    \n",
    "    #Optimize the model\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    ##Test\n",
    "    if (epoch % 100) == 0:\n",
    "        model.eval()\n",
    "        #Get data\n",
    "        x,y = get_batch_data(1000)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pre,attn_output_weights = model(x,src_mask)\n",
    "            pre_idx = torch.argmax(pre,1)\n",
    "            gt = torch.argmax(y[:,1,:],1)\n",
    "            # print('{:.1f}%的样本被预测正确'.format(torch.sum(pre_idx==gt)/10))\n",
    "            print(torch.mean(attn_output_weights,dim=0))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-inf, 0., 0.],\n",
      "        [-inf, 0., 0.],\n",
      "        [-inf, 0., 0.]])\n",
      "52.8%的样本被预测正确; tensor([0.0000, 0.4980, 0.5020])\n",
      "100.0%的样本被预测正确; tensor([0.0000, 0.5983, 0.4017])\n",
      "100.0%的样本被预测正确; tensor([0.0000, 0.6258, 0.3742])\n",
      "100.0%的样本被预测正确; tensor([0.0000, 0.6506, 0.3494])\n",
      "100.0%的样本被预测正确; tensor([0.0000, 0.6634, 0.3366])\n"
     ]
    }
   ],
   "source": [
    "##修改src_mask，遮挡输入序列中第1个位置的信息\n",
    "src_mask =  torch.zeros(MAX_LEN,MAX_LEN)\n",
    "src_mask[:,0] = 1\n",
    "# src_mask[:,2] = 1\n",
    "src_mask = src_mask.masked_fill(src_mask == 1, float('-inf'))\n",
    "print(src_mask)\n",
    "\n",
    "#重新创建模型\n",
    "OUTPUT_IDX = 2\n",
    "model = autoencoder(num_encoder_layers = 1,d_embedding=DIM_EMBEDDING,output_idx=OUTPUT_IDX).to(device)\n",
    "\n",
    "\n",
    "###重新测试transformer_encoder的第OUTPUT_IDX能否看见第MASK_IDX个输入\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01) #0.01\n",
    "criterion = torch.nn.MSELoss()\n",
    "for epoch in range(500):\n",
    "    ##Train\n",
    "    model.train()\n",
    "    \n",
    "    #Get data\n",
    "    x,y = get_batch_data(BATCH_SIZE)\n",
    "    \n",
    "    #Calculate output\n",
    "    optimizer.zero_grad()\n",
    "    pre,attn_output_weights = model(x,src_mask)\n",
    "    \n",
    "    #Calculate loss\n",
    "    loss = criterion(pre,y[:,1,:])\n",
    "    \n",
    "    #Optimize the model\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    ##Test\n",
    "    if (epoch % 100) == 0:\n",
    "        model.eval()\n",
    "        #Get data\n",
    "        x,y = get_batch_data(1000)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pre,attn_output_weights = model(x,src_mask)\n",
    "            pre_idx = torch.argmax(pre,1)\n",
    "            gt = torch.argmax(y[:,1,:],1)\n",
    "            print('{:.1f}%的样本被预测正确'.format(torch.sum(pre_idx==gt)/10),end='; ')\n",
    "            print(torch.mean(attn_output_weights,dim=0))\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
